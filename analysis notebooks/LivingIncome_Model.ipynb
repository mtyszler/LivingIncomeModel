{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Living Income Analysis: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I develop a Restricted Machine Learning model that can predict whether a cocoa farming household in Ghana and CÃ´te d'Ivoire reach a Living Income. The Machine Learning model is restricted to 10 features, to make is real-world application easy and cost-effective\n",
    "\n",
    "I use as starting point the Machine Learning model prepared at [Living Income Analysis: Machine Learning notebook](LivingIncome_MachineLearning.ipynb), which in turn use as starting point the dataset prepared at [Living Income Analysis notebook](LivingIncomeAnalysis.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, specifically I will:\n",
    "\n",
    "* Load the dataset\n",
    "* Extract the top 10 features\n",
    "* Re-fit the best classifier with the restricted dataset, using Grid Search with a stratified Cross-validation\n",
    "* Search broadly using TPOT \n",
    "* Compare performance and choose the best classifier with the restricted data\n",
    "* Export the final model such that it can be used in a web-app and by others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import visuals as vs # this is a slightly modifed code which provided via de Udacity Nanodegree Intro to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data as in the previous notebook [Living Income Analysis: Machine Learning notebook](LivingIncome_MachineLearning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../data/data_ready_for_ML.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the same strategy to fill missing values, so the data is identical:\n",
    "\n",
    "* For categorical variables, use the mode\n",
    "* For numerical variable, use the median\n",
    "\n",
    "The strategy above is also quite robust to outliers, as the mode and median are typically quite stable and not sensitive to extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all features\n",
    "for this_column in list(data.columns):\n",
    "\n",
    "    # check if categorical\n",
    "    if (str(data[this_column].dtype) == 'category') | (str(data[this_column].dtype) == 'object') :\n",
    "        fill_value = data[this_column].mode()\n",
    "\n",
    "    else:\n",
    "        fill_value = data[this_column].median()\n",
    "     \n",
    "    # fill missing values\n",
    "    data[this_column] = data[this_column].fillna(value = fill_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['Living Income Achieved']\n",
    "data.drop(['Living Income Achieved'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand dummies, to maintain the original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shape, which should be `(1377, 838)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Load Best ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model from the previous notebook [Living Income Analysis: Machine Learning notebook](LivingIncome_MachineLearning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = pd.read_pickle('../models/best_overall.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 10 best features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_features = pd.DataFrame({'Feature weight': best_model.feature_importances_}, index=data.columns).sort_values(by='Feature weight', ascending=False).head(n=10)\n",
    "top_10_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.feature_plot(best_model.feature_importances_, data, target, n_features= 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store previous performance metrics for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_adaOptimStrat_accuracy = 0.9652\n",
    "FULL_adaOptimStrat_fscore = 0.8667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADAboost, Full Data Optimized Stratified Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(FULL_adaOptimStrat_accuracy, FULL_adaOptimStrat_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Get Data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract top 10 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_top10 = data[top_10_features.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_top10.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check basic descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_top10.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search best parameters, using stratified shuffle split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_top10, target,\n",
    "                                                    train_size=0.75, test_size=0.25,\n",
    "                                                   random_state = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Re-fit AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare elements for the grid search. using the same parameters as in the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base classifier\n",
    "clf = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Create the parameters list to tune\n",
    "parameters = {'n_estimators':[10, 50, 500, 1200, 1500, 2000],\n",
    "              'learning_rate':[1.0,1.5,2.0]}\n",
    "\n",
    "# Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(fbeta_score, beta= 0.5)\n",
    "\n",
    "# Make sure it is stratified\n",
    "cv_strat = StratifiedShuffleSplit(n_splits=5,\n",
    "                                  test_size=0.2,\n",
    "                                  random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Grid Search instance, using stratified CV\n",
    "grid_obj_strat = GridSearchCV(clf, parameters, scoring = scorer, verbose=2,\n",
    "                             cv = cv_strat, n_jobs = -1)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit_strat = grid_obj_strat.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf_strat = grid_fit_strat.best_estimator_\n",
    "\n",
    "# print best model:\n",
    "print(\"Best model:\")\n",
    "print(best_clf_strat)\n",
    "print(\"------\")\n",
    "\n",
    "# Make predictions \n",
    "best_predictions_strat = best_clf_strat.predict(X_test)\n",
    "\n",
    "# metrics\n",
    "adaOptimStrat_accuracy = accuracy_score(y_test, best_predictions_strat)\n",
    "adaOptimStrat_fscore = fbeta_score(y_test, best_predictions_strat, beta = 0.5)\n",
    "\n",
    "print(\"\\nRestricted Optimized Model with stratification\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(adaOptimStrat_accuracy))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(adaOptimStrat_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the solution is on the limits (`learning rate = 1.0`), let's expand grid parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':[10, 50, 500, 1200, 1300, 1500, 1600, 2000],\n",
    "              'learning_rate':[0.05, 0.1, 0.2, 0.3, 0.5,1.0,1.5,2.0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Grid Search instance, using stratified CV\n",
    "grid_obj_strat = GridSearchCV(clf, parameters, scoring = scorer, verbose=2,\n",
    "                             cv = cv_strat, n_jobs = -1)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit_strat = grid_obj_strat.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf_strat = grid_fit_strat.best_estimator_\n",
    "\n",
    "# print best model:\n",
    "print(\"Best model:\")\n",
    "print(best_clf_strat)\n",
    "print(\"------\")\n",
    "\n",
    "# Make predictions \n",
    "best_predictions_strat = best_clf_strat.predict(X_test)\n",
    "\n",
    "# metrics\n",
    "adaOptimStrat_accuracy = accuracy_score(y_test, best_predictions_strat)\n",
    "adaOptimStrat_fscore = fbeta_score(y_test, best_predictions_strat, beta = 0.5)\n",
    "\n",
    "print(\"\\nRestricted Optimized Model with stratification\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(adaOptimStrat_accuracy))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(adaOptimStrat_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare full with the restricted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADAboost, Full Data Optimized Stratified Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(FULL_adaOptimStrat_accuracy, FULL_adaOptimStrat_fscore))\n",
    "print(\"ADAboost, Restricted Data Optimized Stratified Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(adaOptimStrat_accuracy, adaOptimStrat_fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check broader performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(y_test, best_predictions_strat, beta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, best_predictions_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(best_clf_strat, \n",
    "                      X_test, y_test,\n",
    "                      cmap=plt.cm.Blues,\n",
    "                      normalize = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the best model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = '../models/restricted_ada_best_strat.pkl'\n",
    "pickle.dump(best_clf_strat, open(pkl_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Search broadly using TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer = TPOTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer = TPOTClassifier(verbosity=2,\n",
    "                                     n_jobs = -1,\n",
    "                                    random_state = 32,\n",
    "                                   periodic_checkpoint_folder = \"../models/intermediate_results\",\n",
    "                                    cv = cv_strat,\n",
    "                                    scoring = scorer, max_time_mins = 5\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "print(pipeline_optimizer.score(X_test, y_test))\n",
    "pipeline_optimizer.export('../models/restricted_tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_optimizer.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_pred = pipeline_optimizer.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "tpot_accuracy = accuracy_score(y_test, tpot_pred)\n",
    "tpot_fscore = fbeta_score(y_test, tpot_pred, beta = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare ML models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADAboost, Full Data Optimized Stratified Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(adaOptimStrat_accuracy, adaOptimStrat_fscore))\n",
    "print(\"ADAboost, Restricted Data Optimized Stratified Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(adaOptimStrat_accuracy, adaOptimStrat_fscore))\n",
    "print(\"TPOT Predictor, Restricted Model: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(tpot_accuracy, tpot_fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(y_test, tpot_pred, beta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, tpot_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, tpot_pred, normalize = 'true')\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot(cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save best TPOT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPOT_exported_pipeline = pipeline_optimizer.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_results = TPOT_exported_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(y_test, tpot_results, beta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = '../models/restricted_tpot_best_fitted.pkl'\n",
    "pickle.dump(TPOT_exported_pipeline, open(pkl_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the best models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | ADAboost        |         | TPOT            |         |\n",
    "|-----------|-----------------|---------|-----------------|---------|\n",
    "| accuracy  | 96.5%           |         | 95.6%           |         |\n",
    "|           | _Did not achieve_ | _Achieved_ | _Did not Achieve_ | _Achieved_ |\n",
    "| precision | 98%             | 87%     | 96%             | 92%     |\n",
    "| recall    | 98%             | 87%     | 99%             | 73%     |\n",
    "| f-score   | 98%             | 87%     | 96.6%           | 87.3%   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the ADAboost is more balanced in precision and recall, I prefer this model over the TPOT best pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model =  TPOT_exported_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = 'restricted_best_overall.pkl'\n",
    "pickle.dump(best_model, open(pkl_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides predicting if a farmer reaches a living income, I'm interested in predicting the probability that a farmer reaches a living income. I do this by calibrating the original model `proba`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference see [probability calibration](https://scikit-learn.org/stable/modules/calibration.html#calibration) and this cross-validated [post](https://stats.stackexchange.com/questions/110981/why-is-adaboost-predicting-probabilities-with-so-little-standard-deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 AdaBoostClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the average chance that an observation is classified as reaching a living income (`true`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(best_clf_strat.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is the average probability that an observation reaches the living income (`proba`) before calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(best_clf_strat.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which are clearly not in line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adjust this, we calibrate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_best_clf_strat = CalibratedClassifierCV(best_clf_strat, cv=cv_strat, method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_best_clf_strat.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_best_predictions_strat = calibrated_best_clf_strat.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, calibrated_best_predictions_strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, best_predictions_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are virtually identical. Let's extract the probabilities now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_best_probabilities_strat = calibrated_best_clf_strat.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see the average calibrated chance that an observation is classified as reaching a living income (`true`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(calibrated_best_probabilities_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which now is in line with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(calibrated_best_predictions_strat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 TPOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completness, we do similar to the TPOT best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_tpot = CalibratedClassifierCV(TPOT_exported_pipeline, cv=cv_strat, method='isotonic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_tpot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_tpot_predictions = calibrated_tpot.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, calibrated_tpot_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, tpot_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_tpot_probabilities = calibrated_tpot.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(calibrated_tpot_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(calibrated_tpot_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Web app & write-ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.0 Previous non-technical write-up:\n",
    "\n",
    "See at [medium](https://medium.com/@tyszler.jobs/are-cocoa-farmers-reaching-a-living-income-f7724af574c4?sk=344c18d46a7fd402d3a137061c6ba89a)\n",
    "\n",
    "\n",
    "## 9.1 Non-technical write-up\n",
    "\n",
    "See See at [medium](https://medium.com/@tyszler.jobs/are-cocoa-farmers-reaching-a-living-income-f7724af574c4?sk=344c18d46a7fd402d3a137061c6ba89a)\n",
    "\n",
    "## 9.2 Technical write-up\n",
    "\n",
    "See at [medium](https://medium.com/@tyszler.jobs/are-cocoa-farmers-reaching-a-living-income-f7724af574c4?sk=344c18d46a7fd402d3a137061c6ba89a)\n",
    "\n",
    "## 9.3 Web app\n",
    "\n",
    "See it [live](https://disaster-response-livedemo.herokuapp.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
